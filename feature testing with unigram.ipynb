{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bf5361a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import random\n",
    "import sklearn\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d95c9e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert csv to list of dicts\n",
    "with open('final_df.csv') as f:\n",
    "    data = [{k: v for k, v in row.items()} for row in csv.DictReader(f, skipinitialspace=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5d0700b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uid_x': '255938',\n",
       " 'profile': 'DesolatePsyche',\n",
       " 'anime_uid': '34096',\n",
       " 'text': 'First things first My reviews system is explained on a blog entry Which can be found through my profile   Im going to keep this review more of a opinion of Gintamas overall and then this season specific  Anyhow What I have always loved regarding Gintama is its content of everything I love the comedy its absurd random can be vile dirty sweet anyhow everything Have laughed countless times in this franchise Also the humor they have also is heavily reference based aka parodies of different anime shows manga live stuff real world anime production and so on Anyhow comedyparody side of this franchise i absolutely love  Now nd side of this show is the serious dramas epic battle shounens and so on There are arcs that are fully comedy arcs that are fully serious and mixtures of both Serious side is usually quite dramatic and managed to somewhat tear me up now and then Whilst the action sequences are absolute bliss as well They are just presented in a really cool manner And extra points when sometime the add artistic element  Of course it has its sliceoflife side and pseudoromance Those are weaker side but aint the focus But when sometimes those are the focus then they are done in enjoyable method  To sum up overall gintamas I love the serious side and the silly side  Art and sound voice actingostoped is something I loved in all gintamas across the board They just click with me bliss to look at bliss to hear Music oped specifically are hyped up and upbeat which generally I dont like but gintama is the exception where I love them Art is cool sometimes artistic sometimes simple It has a bit of everything that makes it enjoyable And gore is absolutely satisfying as well well the blood to be specific  Well the serious side of stories in this anime are superb in my opinion or the overall idea of story that moves now and then But wouldnt say comedic side of series story is any worse  When at times comedy side has story Its usually quite enjoyable  Regarding characters I dont have much to say other than that variety stereotypes of offstereotypes is very large As well characters with depth but as well with oddity is as well in great selection Main team Gintoki the lazy dirty slob with superb second side of seriousness and caring about friendsallies Shinpachi the poor straightmanact sidekick who adds great value to the team Kagura the battle race girl that doesnt act like girl almost at all or in another words a dirty slob girl in a sense   Now to this season specific  I found it a bit disappointing Mainly because usually in gintama we have overly serious arcs or fully comedic whilst this one had a bit of mixture of both which somewhat ruined the experience Plus the artistic presentation felt this time around a bit lacking Maybe because of another studio who knows But anyhow it didnt feel as superb Rather than gintama it felt as watching just another good battle shounen anime in modern day with simplified animation  I mean it definitely was not any bad just didnt have the punch Gintama usually has Might be because it was short series maybe because it focused too much moving on with story Itll be shame to see it finalized in this manner  Rest I pretty much loved as usual in Gintama franchise',\n",
       " 'score_x': '8',\n",
       " 'scores': \"{'Overall': '8', 'Story': '8', 'Animation': '8', 'Sound': '10', 'Character': '9', 'Enjoyment': '8'}\",\n",
       " 'text len': '3229',\n",
       " 'uid_y': '34096',\n",
       " 'title': 'Gintama.',\n",
       " 'genre': \"['Action', 'Comedy', 'Historical', 'Parody', 'Samurai', 'Sci-Fi', 'Shounen']\",\n",
       " 'episodes': '12.0',\n",
       " 'members': '139309',\n",
       " 'popularity': '800',\n",
       " 'ranked': '15.0',\n",
       " 'score_y': '8.94'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8e9bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15e1faa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove capitalization (and punctuation)\n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "for d in train_data:\n",
    "    r = ''.join([c for c in d['text'].lower() if not c in punctuation])\n",
    "    ws = r.split()\n",
    "    for w in ws:\n",
    "        wordCount[w] += 1\n",
    "\n",
    "counts = [(wordCount[w], w) for w in wordCount]\n",
    "counts.sort()\n",
    "counts.reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b15b751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1000 most popular words (bag of words)\n",
    "words = [x[1] for x in counts[:1000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "301300bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordId = dict(zip(words, range(len(words))))\n",
    "wordSet = set(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65684caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram\n",
    "def featureUni(datum):\n",
    "    feat = [0]*len(words)\n",
    "    r = ''.join([c for c in datum['text'].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        if w in words:\n",
    "            feat[wordId[w]] += 1\n",
    "    feat.append(int(datum['text len']))\n",
    "    feat.append(1) # offset\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54f07c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [featureUni(d) for d in data]\n",
    "y = [d['score_x'] for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8caae591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 70/30 train/test split. convert y_data (scores) into ints\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "y_train = list(map(int, y_train))\n",
    "y_test = list(map(int, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "cb3fe85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularized regression\n",
    "clf = linear_model.Ridge(1.0, fit_intercept=False) # MSE + 1.0 l2\n",
    "clf.fit(X_train, y_train)\n",
    "theta = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "20dc5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a7ba8e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(predictions, labels):\n",
    "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
    "    return sum(differences) / len(differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8baf6600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2777022317165483"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(predictions, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8abdecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordSort = list(zip(theta[:-1], words))\n",
    "wordSort.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7abfafc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.5129548798418291, 'worst'),\n",
       " (-0.4714228589482305, 'horrible'),\n",
       " (-0.3575173624069023, 'terrible'),\n",
       " (-0.3573067177497681, 'poor'),\n",
       " (-0.30815733250162625, 'mediocre')]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most \"negative\" unigrams\n",
    "wordSort[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0a34024b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.19308189677302412, 'highly'),\n",
       " (0.2150944217225201, 'awesome'),\n",
       " (0.2297299810648875, 'amazing'),\n",
       " (0.2831379563659605, 'outstanding'),\n",
       " (0.4075131819544006, 'masterpiece')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most \"positive\" unigrams\n",
    "wordSort[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98ae854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    correct = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] == labels[i]:\n",
    "            correct += 1\n",
    "    return correct/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9354483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20456634998467668"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert predictions (floats) to int and check accuracy\n",
    "predictions = list(map(int, predictions))\n",
    "accuracy(predictions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75966e6f",
   "metadata": {},
   "source": [
    "## Regularization Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8773922b",
   "metadata": {},
   "source": [
    "no significant MSE and accuracy changes when experimenting with lambda values. similar results when adding review length as feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bec2b2e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l = 0.01, test MSE = 3.2777159243483625, test Acc = 0.20459188885483706\n",
      "l = 0.1, test MSE = 3.2777152730179, test Acc = 0.20459188885483706\n",
      "l = 1, test MSE = 3.277708763785827, test Acc = 0.20459188885483706\n",
      "l = 10, test MSE = 3.277644077742966, test Acc = 0.20451527224435592\n",
      "l = 100, test MSE = 3.277036901311336, test Acc = 0.204387577893554\n",
      "l = 1000, test MSE = 3.2741638520471628, test Acc = 0.2042343446725917\n",
      "l = 10000, test MSE = 3.3308302315776435, test Acc = 0.2013229134743079\n"
     ]
    }
   ],
   "source": [
    "bestModel = None\n",
    "bestVal = None\n",
    "bestLamb = None\n",
    "bestAcc = None\n",
    "\n",
    "ls = [0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "errorTrain = []\n",
    "errorTest = []\n",
    "\n",
    "for l in ls:\n",
    "    model = sklearn.linear_model.Ridge(l)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictTrain = model.predict(X_train)\n",
    "    MSEtrain = sum((y_train - predictTrain)**2)/len(y_train)\n",
    "    errorTrain.append(MSEtrain)\n",
    "    predictTest = model.predict(X_test)\n",
    "    MSEtest = sum((y_test - predictTest)**2)/len(y_test)\n",
    "    predictTest = list(map(int, predictTest))\n",
    "    accTest = accuracy(predictTest, y_test)\n",
    "    errorTest.append(MSEtest)\n",
    "    print(\"l = \" + str(l) + \", test MSE = \" + str(MSEtest) + \", test Acc = \" + str(accTest))\n",
    "    if bestVal == None or MSEtest < bestVal:\n",
    "        bestVal = MSEtest\n",
    "        bestModel = model\n",
    "        bestLamb = l\n",
    "    if bestAcc == None or accTest > bestAcc:\n",
    "        bestAcc = accTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "b0fb1c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20459188885483706"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f8069",
   "metadata": {},
   "source": [
    "## tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "de5a9eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5a186f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(reviewText):\n",
    "    nopunc = [i for i in reviewText if i not in string.punctuation]\n",
    "    nopunc_text = ''.join(nopunc)\n",
    "    return [i for i in nopunc_text.split() if i.lower() not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "03e62139",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.42      0.28      0.34       882\n",
      "          10       0.52      0.63      0.57      7319\n",
      "           2       0.25      0.06      0.09       975\n",
      "           3       0.25      0.21      0.23      1733\n",
      "           4       0.25      0.10      0.14      1836\n",
      "           5       0.22      0.16      0.19      2584\n",
      "           6       0.25      0.21      0.23      3767\n",
      "           7       0.29      0.32      0.31      5738\n",
      "           8       0.31      0.38      0.34      6949\n",
      "           9       0.35      0.36      0.36      7372\n",
      "\n",
      "    accuracy                           0.35     39156\n",
      "   macro avg       0.28      0.25      0.25     39156\n",
      "weighted avg       0.34      0.35      0.34     39156\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1248: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Benchmark model - multinomial logistic regression\n",
    "pipeline = Pipeline([\n",
    "    ('Tf-Idf', TfidfVectorizer(ngram_range=(1,2), analyzer=text_process)),\n",
    "    ('classifier', linear_model.LogisticRegression(solver='newton-cg', multi_class='multinomial'))\n",
    "])\n",
    "X1 = [d['text'] for d in data]\n",
    "y1 = [d['score_x'] for d in data]\n",
    "review_train1, review_test1, label_train1, label_test1 = train_test_split(X1, y1, test_size=0.3, random_state=0)\n",
    "pipeline.fit(review_train1, label_train1)\n",
    "pip_pred1 = pipeline.predict(review_test1)\n",
    "print(metrics.classification_report(label_test1,pip_pred1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b24b5b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.35220655838185716"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = list(map(int, pip_pred1))\n",
    "accuracy(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0257dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (clean)",
   "language": "python",
   "name": "python3_clean"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
